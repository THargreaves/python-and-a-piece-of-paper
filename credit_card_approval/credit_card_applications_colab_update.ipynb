{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "credit_card_applications.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soUB9wPD_KcA",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Credit Card Applications \n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook accompanies the talk I gave in Februrary 2020 to Warwick Finance Society titled 'Python and a Piece of Paper'. The presentation (which provides background on this notebook) can be found on the [GitHub repository](https://github.com/THargreaves/python-and-a-piece-of-paper) for this. If you enjoyed the talk and/or appreciate this live notebook to try the code yourself, please do give the notebook a star (GitHub's equivalent of a like).\n",
        "\n",
        "You can run the code by selecting a cell of the notebook and either using `Ctrl-Enter` or clicking the play icon next to the cell. Each cell depends on the last so make sure they are ran in order. If you want to undo the effect of a cell, select it and use `Ctrl-F8` or select `Runtime > Run Before` in the top menu to run all cells up to (but not including) itself.\n",
        "\n",
        "\n",
        "If there are any issues, feel free to reach out to me on [LinkedIn](https://www.linkedin.com/in/tim-hargreaves/). For more interesting Data Science projects, check out [my blog](https://www.ttested.com/).\n",
        "\n",
        "**Enjoy!**\n",
        "\n",
        "_Note: The code in this notebook is slightly different from that in the presentation. This is because I wrote the code and slides using my work laptop which has an outdated version of certain packages (blaim IT!) whereas Google Colab uses the latest versions. It was easier to change the code here than in my slides so that's how it will have to be._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkYrwWD95JgO",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Here we import various Python packages and modules to offer additionally functionality to our code without any added effort."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la53dGDHH8SB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V54IMZcu6BN9",
        "colab_type": "text"
      },
      "source": [
        "## Import\n",
        "\n",
        "We download the dataset from the UCI Machine Learning repository and import it into Python. We do a bit of minor housekeeping too, such as removing the ZIP code column (too much fuss to process) and splitting the predictors and labels.\n",
        "\n",
        "The original dataset and some information about it can be found [here](https://archive.ics.uci.edu/ml/datasets/credit+approval). The probable column names are taken from [this](http://rstudio-pubs-static.s3.amazonaws.com/73039_9946de135c0a49daa7a0a9eda4a67a72.html) blog post ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0Li4J-56Rnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crx = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/' +\n",
        "                  'credit-screening/crx.data', header=None)\n",
        "\n",
        "# add probable column names\n",
        "crx.columns = [\n",
        "    'sex', 'age', 'debt', 'married', 'bank_customer', 'education', 'ethnicity', \n",
        "    'year_employed', 'prior_default', 'employed', 'credit_score',\n",
        "    'drivers_license', 'citizen', 'zip_code', 'income', 'approved',\n",
        "]\n",
        "\n",
        "# remove unhelpful features\n",
        "crx = crx.drop(['zip_code'], axis=1)\n",
        "\n",
        "# extract label\n",
        "lab = (crx['approved'] == '+').to_numpy()\n",
        "crx = crx.drop(['approved'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGFwd7og72oj",
        "colab_type": "text"
      },
      "source": [
        "You can have a look at the imported dataset by adding a code block, typing `crx` and clicking the play button."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOTyg7d_67jH",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQZ6FmIM7EuV",
        "colab_type": "text"
      },
      "source": [
        "### Impute Missing Values\n",
        "\n",
        "Most machine learning models can't handle missing values by themselves so we have to pre-process them. One approach is just to throw out any columns or rows with a lot of missing values but this is wasteful if there are a lot of missing values. A better approach is to replace missing numeric values with column means and missing string values with the most common category.\n",
        "\n",
        "We also add new columns representing which predictors each person was missing data forâ€”perhaps some missing data is caused by the applicant trying to hide information that makes them look back?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QBu5DKL7DDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# missing values saved as '?'\n",
        "crx = crx.replace(['?'],np.NaN)\n",
        "# fix column datatypes\n",
        "crx['age'] = crx['age'].astype('float')\n",
        "# keep track of which columns are missing data\n",
        "for name, values in crx.iteritems():\n",
        "    is_na = values.isna()\n",
        "    if any(is_na):\n",
        "        crx[f'{name}_is_na'] = is_na\n",
        "# replace missing numeric values with column means\n",
        "crx.fillna(crx.mean(), inplace=True)\n",
        "# replace missing string values with column modes\n",
        "for name, values in crx.iteritems():\n",
        "    if values.dtype == 'object':\n",
        "        crx[name] = values.fillna(values.value_counts().index[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw2K1nq28XOo",
        "colab_type": "text"
      },
      "source": [
        "You can use `crx.iloc[[206, 248], :]` before and after running this code block to see the effect of imputation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa20UOdw8b00",
        "colab_type": "text"
      },
      "source": [
        "### Encode Non-Numeric Features\n",
        "\n",
        "Machine learning algorthms prefer their input as numbers rather than text. We therefore change the representation of text columns using a technique called [One-hot Encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HxC0XrI8H2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split numeric and non-numeric features\n",
        "numeric = crx.select_dtypes(exclude='object')\n",
        "string = crx.select_dtypes(include='object')\n",
        "enc = OneHotEncoder()\n",
        "string_enc = enc.fit_transform(string)\n",
        "crx_enc = np.hstack((numeric.to_numpy(), string_enc.todense()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuwB_MA882X_",
        "colab_type": "text"
      },
      "source": [
        "The output of this code block is an object called `crx_enc`. You can print this in the same way you did for `crx` although the result will now just be a block of numbers without column names. Not as pretty, I know, but it makes Python happy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlOQQGmj9Ly3",
        "colab_type": "text"
      },
      "source": [
        "### Normalise Inputs\n",
        "\n",
        "Scale values so that they lie between zero and one. This prevents machine learning models from unfairly favouring predictors that have a large range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QXLT4_i9LFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scal = MinMaxScaler()\n",
        "crx_enc = scal.fit_transform(crx_enc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PA1oDeo9boj",
        "colab_type": "text"
      },
      "source": [
        "To see the impact of this change, use `plt.hist(crx_enc[:, 2])` before and after scaling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGeARjiQ9-dj",
        "colab_type": "text"
      },
      "source": [
        "## Build Model\n",
        "\n",
        "We build a model by sequentially stacking layers of _neurons_. We then compile the model by telling it how it should 'learn'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdeDaHVI-DI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    model = Sequential([\n",
        "        Dense(16, activation='relu', \n",
        "              input_shape=(crx_enc.shape[1],)),\n",
        "        Dense(8, activation='relu'),\n",
        "        Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', \n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXI1l0Ie-S5M",
        "colab_type": "text"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol0HnmIy-W60",
        "colab_type": "text"
      },
      "source": [
        "### Fit Model\n",
        "\n",
        "We _fit_ the model to our data. This is essentially the time in which we teach the model. Don't worry if this block takes a few minutes to runâ€”learning takes time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpXOensr-bEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn = KerasClassifier(build_fn=build_model, epochs=10, batch_size=32, verbose=0)\n",
        "kfold = KFold(n_splits=30)\n",
        "results = cross_val_score(nn, crx_enc, lab, cv=kfold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b2b4-CwBnCR",
        "colab_type": "text"
      },
      "source": [
        "If the code is taking a _really_ long time to run (10 minutes or more) then it may be the case that the Python kernel has died. To fix this use `Ctrl+M` to restart the runtime. You will then have to run the code blocks from the start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xk-_Fmc-ZOU",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Performance\n",
        "\n",
        "We evaluate our model by trying to predict for data that the model has **never seen before**. This way we can be sure that the model is actually learning and not just memorising the examples it's seen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuPdcapF-amF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9257eb86-2555-409b-e1ff-eaaed7645564"
      },
      "source": [
        "acc = results.mean()\n",
        "print(f\"Model accuracy is {round(acc * 100, 2)}%\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model accuracy is 84.06%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBb5mQ1GB9M5",
        "colab_type": "text"
      },
      "source": [
        "You may not obtain the exact same accuracy as in the presentation due to the random nature of the algorithms used and hardware differences. It should be in the same ball park though. It appears from other models this dataset that ~85% is about as good as you can get without more predictors/observations/inside knowledge. Not bad for a quick bodge-of-a-solution!"
      ]
    }
  ]
}